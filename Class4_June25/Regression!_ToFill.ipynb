{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a notebook to show some simple regression algorithms and metrics. \n",
    "\n",
    "#Author: Viviana Acquaviva\n",
    "\n",
    "#License: BSD but really should be TBD - just be nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "#Just to make our life easier!\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression problems: in which we'd like to predict a continuous quantity, not a class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our life simple, we will recycle one of our previous problems (the LAE vs OII classification) and re-cast it as a regression problem. Obviously it's not the best example, we will see another one later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('LAE_OII_CCA.txt', sep = '\\t', comment = '#')\n",
    "data.columns =['type', 'wavelength', 'ELflux', 'continuum', 'EW']\n",
    "seldata = data[(np.abs(stats.zscore(data.drop(['type'],axis=1))) < 3).all(axis=1)]\n",
    "le = LabelEncoder()\n",
    "newcol = le.fit_transform(seldata.type.values)\n",
    "seldata.ix[:,'type'] = newcol\n",
    "X, y = seldata.drop('type',axis=1), seldata.type\n",
    "normalized_X = (X - X.mean())/X.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: What is the main implementation difference between classification and regression problems?\n",
    "\n",
    "......\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This line shows all the possible scoring functions accepted by sklearn.\n",
    "\n",
    "sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a regression model using Random Forests, picking a scoring parameter from the list above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill fill fill\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another one that is used often is the 'r2' score. It's often called the coefficient of determination, usually denoted as RÂ². It represents the proportion of variance (of y) that has been explained by the predictions. It's read as a square but it can be negative if the model is worse than a predictor of the mean :/\n",
    "\n",
    "It's defined as \n",
    "\n",
    "$$ R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Fill fill fill\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Even if 1 is the best value, the R2 score is not a percentage! So how can we understand how good the predictions are?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good start is to visualize the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y, bins = 20, alpha = 0.5, label = 'True')\n",
    "plt.hist(ypred, bins = 20, alpha = 0.5, label = 'Pred')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also take a look at a scatter plot..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y,ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this \"regressor\" is doing, e.g. how the points with intermediate probability look like, in a 2D projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "plt.scatter(normalized_X['EW'], normalized_X['continuum'], c = y)\n",
    "plt.xlim([-0.25,-0.2])\n",
    "plt.ylim([-0.3,1])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "\n",
    "plt.scatter(normalized_X['EW'], normalized_X['continuum'], c = ypred)\n",
    "plt.xlim([-0.25,-0.2])\n",
    "plt.ylim([-0.3,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know how to play with parameters using Grid Search CV, so no big surprise here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a little CV optimization, note that we have lost our class weight parameter though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvmethod = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "\n",
    "parameters = {'max_depth':[3,5,8], \\\n",
    "              'max_features': [2,4], 'n_estimators':[10,20,50]}\n",
    "\n",
    "nmodels = np.product([len(el) for el in parameters.values()])\n",
    "\n",
    "model = GridSearchCV(RandomForestRegressor(), parameters, cv = cvmethod, \\\n",
    "                     scoring = 'neg_mean_absolute_error', \\\n",
    "    verbose = 1, n_jobs = 4)\n",
    "start = time.time()\n",
    "model.fit(normalized_X, y)\n",
    "stop = time.time()\n",
    "print('Best params, best score:', \"{:.4f}\".format(model.best_score_), model.best_params_),\n",
    "print('Time per model (s):', \"{:.4f}\".format((stop-start)/float(nmodels*4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(model.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df[['params', 'mean_test_score', 'mean_train_score', 'mean_fit_time']].sort_values(by = 'mean_test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discuss which parameters are best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is best, classification or regression?\n",
    "\n",
    "Many classification problems can be turned into regression problems. So, which one should we pick?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_RFC = cross_val_predict(...)\n",
    "\n",
    "predicted_RFR = cross_val_predict(....)\n",
    "\n",
    "plt.hist(predicted_RFC,alpha = 0.5, color='green',label = 'RF Classifier')\n",
    "plt.hist(predicted_RFR,alpha = 0.5, label = 'RF Regressor')\n",
    "plt.hist(y+0.05,alpha = 0.5, label = 'True')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tricks: \n",
    "\n",
    "1) Look at input data, if they really look like classes then you should use a classifier.\n",
    "\n",
    "2) You can't compute the accuracy if the target is continuous variable, but you can compute other metric scores if the target is a class! However I am not sure if this helps (see above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.mean_absolute_error(y, predicted_RFC))\n",
    "\n",
    "print(metrics.mean_absolute_error(y, predicted_RFR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last time, we saw that many classifiers also have a \"predict.proba\" method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the above compare to the \"predict.proba\" property of classifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X,y,random_state=5)\n",
    "\n",
    "probas = RandomForestClassifier(n_estimators=20,max_depth=5).fit(Xtrain, ytrain).predict_proba(Xtest) #doing only on one fold\n",
    "\n",
    "probas2 = RandomForestRegressor(n_estimators=20,max_depth=5).fit(Xtrain, ytrain).predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(probas[:,1],alpha = 0.5, color='green',label = 'RF Classifier')\n",
    "plt.hist(probas2,alpha = 0.5, label = 'RF Regressor')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like under the hood, classifiers with a \"predict_proba\" attribute are using a regression model to fit the probability that the object belongs to a given (positive for a binary classifier) class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to estimate errors in ML regression predictions?\n",
    "\n",
    "My answer: Bootstrap. This doesn't take into account the \"architecture\" error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a simple example. We know that EW, continuum and emission line flux are not independent. So let's try to predict one from the other two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = normalized_X[['EW','continuum']]\n",
    "target = normalized_X['ELflux'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()\n",
    "\n",
    "ypred = cross_val_predict(model, features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a couple of evaluation metrics, and also make a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build 10 bootstrap samples where each point is randomly scattered by an amount proprtional to it observational arror (here 1%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_pred = np.empty((len(target),10))\n",
    "\n",
    "for i in range(10):\n",
    "    newEW = features.EW + np.random.normal(0, np.abs(0.01*features.EW)) \n",
    "    newcont = features.continuum + np.random.normal(0, np.abs(0.01*features.continuum)) \n",
    "    feat = np.vstack([newEW,newcont]).T\n",
    "    bootstrap_pred[:,i] = cross_val_predict(model, feat, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This prints the prediction + its standard deviation, which can be used as proxy for uncertainty\n",
    "\n",
    "for i in range(len(target)):\n",
    "    print('%.3f \\t %.3f \\t %.3f' % (target[i], ypred[i], bootstrap_pred[i,:].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And finally, we can make a scatter plot with error bars.\n",
    "\n",
    "plt.errorbar(target, ypred, yerr=bootstrap_pred.std(axis = 1), fmt='o', markersize = 2, c= 'k',ecolor='lightgray', elinewidth=1)\n",
    "plt.plot(np.arange(10),np.arange(10),c='r')\n",
    "plt.xlabel('True flux');\n",
    "plt.ylabel('Est. flux');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- In regression problems, we predict quantities and not classes.\n",
    "\n",
    "- We can use the same algorithms, but the evaluation metric changes and is typically a tracer of how close we are to the true values.\n",
    "\n",
    "- We also saw how we can use bootstrap to include individual errors in our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further reading: For a complete worked example you can see the \"mass-luminosity-color-metallicity\" notebook at\n",
    "\n",
    "https://github.com/vacquaviva/Metallicity_Estimation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
