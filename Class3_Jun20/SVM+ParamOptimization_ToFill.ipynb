{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a notebook to show how a support vector machine  \n",
    "# performs on the simplified classic digits data set. \n",
    "\n",
    "#Author: Viviana Acquaviva\n",
    "\n",
    "#License: BSD but really should be TBD - just be nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC #Support Vector Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder #for exercises only\n",
    "\n",
    "#Just to make our life easier!\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use a toy data set (digits)\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "data = scale(digits.data) #normalization\n",
    "\n",
    "n_samples, n_features = data.shape\n",
    "n_digits = len(np.unique(digits.target))\n",
    "labels = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 10, figsize=(8, 8),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's do a simple cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizing SVC: THIS IS NOT YET NESTED CV\n",
    "\n",
    "parameters = {'kernel':['rbf','poly'], \\\n",
    "              'gamma':[0.01, 0.1, 0.5], 'C':[1,10,100], 'degree': [2,4,6]}\n",
    "nmodels = np.product([len(el) for el in parameters.values()])\n",
    "start = time.time()\n",
    "model = GridSearchCV(SVC(), parameters, cv = StratifiedKFold(n_splits=4, shuffle=True), \\\n",
    "                     verbose = 1, n_jobs = 4)\n",
    "model.fit(data, labels)\n",
    "stop = time.time()\n",
    "print('Best params, best score:', \"{:.4f}\".format(model.best_score_), \\\n",
    "      model.best_params_)\n",
    "print('Time per model (s):', \"{:.4f}\".format((stop-start)/float(nmodels*4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get some insights on how the models are behaving, we can take a look at the object \"cv_results_\", after selecting some important columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(model.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.options.display.max_colwidth = 100 #Another acceptable syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to list the parameters, their train/test scores, and the timings, ranked by the test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now deploy nested CV!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Important! COMMENT\n",
    "\n",
    "#Outer k-fold:\n",
    "    \n",
    "outercv = StratifiedKFold(n_splits=5, shuffle=True) #creates 5 disjoint splits\n",
    "\n",
    "innercv = StratifiedKFold(n_splits=4, shuffle=True) #creates 4 disjoint splits\n",
    "\n",
    "i=0\n",
    "\n",
    "winning_model_scores = []\n",
    "\n",
    "for train_index, test_index in outercv.split(data,labels): #This runs the outer cross validation\n",
    "    \n",
    "    i+=1\n",
    "    \n",
    "    print('Fold ' ,i, 'outer cross validation')\n",
    "    \n",
    "    X_train = data[train_index] #\"yellow\" training set\n",
    "    y_train = labels[train_index]\n",
    "    \n",
    "    X_test = data[test_index]\n",
    "    y_test = labels[test_index]\n",
    "    \n",
    "    #optimizing SVC: this replaces the inner loop!\n",
    "    \n",
    "    parameters = {'kernel':['rbf','poly'], \\\n",
    "              'gamma':[0.01, 0.1, 0.5], 'C':[1,10,100], 'degree': [2,4,6]}\n",
    "    nmodels = np.product([len(el) for el in parameters.values()])\n",
    "    start = time.time()\n",
    "    model = GridSearchCV(SVC(), parameters, cv = innercv, scoring = 'accuracy', \\\n",
    "                     verbose = 1, n_jobs = 4)\n",
    "    model.fit(X_train, y_train)\n",
    "    stop = time.time()\n",
    "    print('Best params, best score:', \"{:.4f}\".format(model.best_score_), model.best_params_)\n",
    "    print('Time per model (s):', \"{:.4f}\".format((stop-start)/float(nmodels*4)))\n",
    "\n",
    "    #Compute test scores with optimal parameters on outer i-th test fold\n",
    "    \n",
    "    winner = model.best_estimator_\n",
    "    \n",
    "    winner.fit(X_train, y_train)\n",
    "    \n",
    "    ypred = winner.predict(X_test)\n",
    "    \n",
    "    #Note the scoring appears explicitly here!\n",
    "    \n",
    "    winning_model_scores.append(metrics.accuracy_score(y_test,ypred)) #append this to the outer cv results\n",
    "    \n",
    "print('The average of the winning model scores (i.e. the generalization error) is', \\\n",
    "      np.mean(winning_model_scores), np.std(winning_model_scores) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: The performance that you can expect from your classifier on new data is ....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### However, your final model will be the one found by using grid-search CV on the full data set, picking the winning parameters, and its anticipated generalization error will be the one given above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: Grid Search is often too expensive, can use Random Search. \n",
    "    \n",
    "After checking learning curves, one can also select a smaller data set for the same reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1: How does the SVM compare with RF/GBT on the LAE/OII data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2: Play with the original MNIST data set (often used for benchmarking of models, see http://yann.lecun.com/exdb/mnist/). A reasonably wide Grid Search takes a couple of days because there are 70,000 images and are 28x28 (784 features). You can play with the code below that already knows about great parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST classification using Support Vector algorithm with RBF kernel\n",
    "# Author: Krzysztof Sopyla <krzysztofsopyla@gmail.com>\n",
    "# https://ksopyla.com\n",
    "# License: MIT\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "dataset2 = datasets.fetch_mldata(\"MNIST Original\")\n",
    "\n",
    "#Note: IF THE ABOVE DOESN'T WORK, you need to download the MNIST data set (.mat file) e.g. from\n",
    "#https://github.com/amplab/datascience-sp14/blob/master/lab7/mldata/mnist-original.mat\n",
    "#Then place it in the \"~/scikit_learn_data/mldata\" directory\n",
    "\n",
    "#data field is 70k x 784 array, each row represents pixels from 28x28=784 image\n",
    "images = dataset2.data\n",
    "targets = dataset2.target\n",
    "\n",
    "#---------------- classification begins -----------------\n",
    "#scale data for [0,255] -> [0,1]\n",
    "#sample smaller size for testing\n",
    "rand_idx = np.random.choice(images.shape[0],30000) #change number here to see accuracy and time scaling\n",
    "X_data =images[rand_idx]/255.0\n",
    "Y      = targets[rand_idx]\n",
    "\n",
    "#full dataset classification\n",
    "#X_data = images/255.0\n",
    "#Y = targets\n",
    "\n",
    "#split data to train and test\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, Y, test_size=0.15, random_state=42)\n",
    "\n",
    "\n",
    "################ Classifier with good params ###########\n",
    "# Create a classifier: a support vector classifier\n",
    "\n",
    "param_C = 5\n",
    "param_gamma = 0.05\n",
    "classifier = svm.SVC(C=param_C,gamma=param_gamma)\n",
    "\n",
    "#We learn the digits on train part\n",
    "start_time = dt.datetime.now()\n",
    "print('Start learning at {}'.format(str(start_time)))\n",
    "classifier.fit(X_train, y_train)\n",
    "end_time = dt.datetime.now() \n",
    "print('Stop learning {}'.format(str(end_time)))\n",
    "elapsed_time= end_time - start_time\n",
    "print('Elapsed learning {}'.format(str(elapsed_time)))\n",
    "\n",
    "\n",
    "########################################################\n",
    "# Now predict the value of the test\n",
    "expected = y_test\n",
    "predicted = classifier.predict(X_test)\n",
    "\n",
    "#show_some_digits(X_test,predicted,title_text=\"Predicted {}\")\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(expected, predicted)))\n",
    "      \n",
    "cm = metrics.confusion_matrix(expected, predicted)\n",
    "print(\"Confusion matrix:\\n%s\" % cm)\n",
    "\n",
    "print(\"Accuracy={}\".format(metrics.accuracy_score(expected, predicted)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
