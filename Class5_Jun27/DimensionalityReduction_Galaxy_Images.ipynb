{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple notebook to do PCA on SDSS spectra and galaxy images.\n",
    "\n",
    "#Author: Viviana Acquaviva (license: BSD), see also other sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "\n",
    "import skimage\n",
    "from skimage.transform import resize, rescale\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) and similar algorithms are used for dimensionality reduction in data-intensive sciences. It finds the most interesting linear combinations of attributes, so that high-dimensional data can be visualized in a 2D or 3D plot. Scikit-learn has methods to compute PCA and several variants. Classic PCA has tough complexity $\\mathcal{O}[N^3].$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example from \n",
    "\n",
    "https://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/tutorial/astronomy/dimensionality_reduction.html#sdss-spectral-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('spec4000_corrected.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelengths = data['wavelengths']\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "labels = data['labels'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can plot a bunch of examples\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(10):\n",
    "    l = plt.plot(wavelengths, X[i] + 20 * i)\n",
    "    c = l[0].get_color()\n",
    "    plt.text(6800, 2 + 20 * i, labels[y[i]], color=c)\n",
    "\n",
    "plt.subplots_adjust(hspace=0)\n",
    "plt.xlabel('wavelength (Angstroms)')\n",
    "plt.ylabel('flux + offset')\n",
    "plt.title('Sample of Spectra');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And some more representative examples\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "for i_class in (2, 3, 4, 5, 6):\n",
    "    i = np.where(y == i_class)[0][0]\n",
    "    l = plt.plot(wavelengths, X[i] + 20 * i_class)\n",
    "    c = l[0].get_color()\n",
    "    plt.text(6800, 2 + 20 * i_class, labels[i_class], color=c)\n",
    "\n",
    "plt.subplots_adjust(hspace=0)\n",
    "plt.xlabel('wavelength (Angstroms)')\n",
    "plt.ylabel('flux + offset')\n",
    "plt.title('Sample of Spectra');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Perform PCA\n",
    "\n",
    "X = preprocessing.normalize(X)\n",
    "\n",
    "pca = decomposition.PCA(n_components=50, random_state=0)\n",
    "\n",
    "X_proj = pca.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "#\n",
    "#  plot PCA eigenspectra\n",
    "#\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "l = plt.plot(wavelengths, pca.mean_ - 0.15)\n",
    "c = l[0].get_color()\n",
    "plt.text(7000, -0.16, \"mean\", color=c)\n",
    "\n",
    "for i in range(4):\n",
    "    l = plt.plot(wavelengths, pca.components_[i] + 0.15 * i)\n",
    "    c = l[0].get_color()\n",
    "    plt.text(7000, -0.01 + 0.15 * i, \"component %i\" % (i + 1), color=c)\n",
    "plt.ylim(-0.2, 0.6)\n",
    "plt.xlabel('wavelength (Angstroms)')\n",
    "plt.ylabel('scaled flux + offset')\n",
    "plt.title('Mean Spectrum and Eigen-spectra')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interpret the eigenvectors as the \"basis\" that explains most of the variability in the data. Therefore, these methods are useful not only to reduce the size of the data set, but also to build intuition on the most important features.\n",
    "\n",
    "How can we know if this works? Let's reverse-engineer the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xrec = pca.inverse_transform(X_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.plot(wavelengths, X[i], label = 'orig', c = 'k')\n",
    "    plt.plot(wavelengths, Xrec[i], '--', label = 'new', c = 'r')\n",
    "    plt.legend()\n",
    "plt.xlabel('wavelength (Angstroms)');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of the ideal number, we can plot the \"explained_variance_ratio\" property of the PCA decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now take a look at images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes < 1 minute, let's use non-bubbled images\n",
    "\n",
    "images = []\n",
    "for i in range(200):\n",
    "    img =skimage.io.imread('Image_'+str(i)+'.png')\n",
    "    img_resized = resize(img,(100,100))\n",
    "    length = np.prod(img_resized.shape)\n",
    "    img_resized = np.reshape(img_resized,length)\n",
    "    images.append(img_resized)\n",
    "    \n",
    "images = np.vstack(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did it on the RGB channels separately. I'm not sure if it's optimal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_images = images.reshape(200, -1,  3)[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling PCA on images\n",
    "\n",
    "estimator = decomposition.PCA(n_components=100)                       \n",
    "r_images_PCA = estimator.fit_transform(r_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This tell us about the dimensionality reduction we have achieved.\n",
    "\n",
    "r_images_PCA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = estimator.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the first 50 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 10, figsize=(12, 6),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow((estimator.components_[i].reshape(100, 100)), cmap='bone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here if we see an obvious optimal number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(estimator.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now reconstruct the original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_projected = estimator.inverse_transform(r_images_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots(2, 10, figsize=(15, 5),\n",
    "                       subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                       gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "for i in range(10):\n",
    "    ax[0, i].imshow(r_images[i].reshape(100, 100), cmap='gray')\n",
    "    ax[1, i].imshow(r_projected[i].reshape(100, 100), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do it for the three channels at once and then join them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = decomposition.PCA(n_components=100) \n",
    "\n",
    "r_images = images.reshape(200, -1,  3)[:,:,1]                     \n",
    "estimator.fit(r_images)\n",
    "r_images_PCA = estimator.fit_transform(r_images)\n",
    "r_projected = estimator.inverse_transform(r_images_PCA)\n",
    "\n",
    "g_images = images.reshape(200, -1,  3)[:,:,1]                     \n",
    "estimator.fit(g_images)\n",
    "g_images_PCA = estimator.fit_transform(g_images)\n",
    "g_projected = estimator.inverse_transform(g_images_PCA)\n",
    "\n",
    "b_images = images.reshape(200, -1,  3)[:,:,2]                     \n",
    "estimator.fit(b_images)\n",
    "b_images_PCA = estimator.fit_transform(b_images)\n",
    "b_projected = estimator.inverse_transform(b_images_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots(2, 5, figsize=(50, 20),\n",
    "                       subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                       gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "for i in range(5,10):\n",
    "    ax[0, i-5].imshow((np.dstack([r_images[i].reshape(100, 100)*255, g_images[i].reshape(100, 100)*255, \n",
    "        b_images[i].reshape(100,100)*255]).astype(np.uint8)))\n",
    "    ax[1, i-5].imshow((np.dstack([r_projected[i].reshape(100, 100)*255, g_projected[i].reshape(100, 100)*255, \n",
    "        b_projected[i].reshape(100,100)*255]).astype(np.uint8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction techniques are useful both to build understanding of what's in the data and to make sizes more manageable.\n",
    "\n",
    "Im many cases, it can be used as a pre-processing step for clustering analyses. For example in the Example # 2 of:\n",
    "\n",
    "https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.11-K-Means.ipynb\n",
    "\n",
    "Some nonlinear techniques (e.g. Kernel PCA, t-SNE, SOM...) are better at capturing the variance.\n",
    "\n",
    "See for example:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/manifold.html#manifold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
